---
layout: post
title: Basic Knowledge of Machine Learning
category: DA
tags: DA
keywords:
description:
---

参考自《机器学习》，周志华

## 线性模型

1. 线性回归

    给定数据集 $D = \lbrace(x_{1}, y_{1}), (x_{2}, y_{2}), ..., (x_{m}, y_{m})\rbrace$, 其中$x_{i} = (x_{i1}; x_{i2}; ...; x_{id}), y_{i} \in R$. "线性回归"试图学得一个线性模型以尽可能准确地预测实值输出标记。

2. 逻辑回归
   
    找一个单调可微函数将分类任务的真实标记$y$与线性回归模型的预测值联系起来。

    比如二分类任务用sigmoid函数，多分类任务用softmax函数。

3. 类别不平衡问题
   
   - 欠采样：去除一些反例是的正、反例数目接近。
   - 过采样：增加一些正例使得正、反例数目接近。
   - 阈值移动：直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将以下公式嵌入到决策过程中。

$$\cfrac{y\prime}{1 - y\prime} = \cfrac{y}{1 - y} * \cfrac{m^-}{m^+}$$

其中，$m^+$表示正例数目，$m^-$表示反例数目，$\cfrac{y}{1 - y}$反应了正例可能性与反例可能性的比值，$\cfrac{y\prime}{1 - y\prime}$表示经过阈值移动后，正例可能性与反例可能性的比值。

例如，在我们用$y = w^T x + b$对新样本进行分类时，事实上是在用预测出的$y$值与一个阈值进行比较，例如通常在$y > 0.5$时判别为正例（此时$\cfrac{y}{1 - y} > 1$），否则为反例。$y$实际表达了正例的可能性。


## 决策树
待写...



## 神经网络

- 神经网络中最基本的成分是神经元模型。神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过“激活函数”处理以产生神经元的输出。理想中的激活函数是阶跃函数，它将输入值映射为输出值"0"或"1"。然而阶跃函数具有不连续、不光滑等不太好的性质，因此实际常用relu函数作为激活函数。
  
- 感知机(Perceptron)由**两**层神经元组成，只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，其学习能力非常有限。感知机可以解决线性可分问题，例如“或”、“与”、“非”；但是不能解决“异或”这样简单的非线性可分问题。

- 多层前馈神经网络：要解决非线性可分问题，需考虑使用多层功能神经单元。例如“多层前馈神经网络”，其每层神经元只与下一层神经元互连，神经元之间不存在同层连接，也不存在跨层连接。

- 误差逆传播(Error BackPropagation,简称BP)算法：
    
    1. BP算法基于梯度下降(gradient descent)策略，以目标的负梯度方向以一定的步长对参数进行调整，这里步长即指学习率。
    2. 任意参数的v的更新估计式为$v = v + \Delta v$
 
- 学习率 $lr \in (0, 1)$ 控制着算法每一轮迭代中的更新步长，若太大则容易震荡，太小收敛速度又会过慢。有时为了精细调节，可对权重和偏置设置不同的学习率。比如偏置的学习率设为权重的学习率的2倍。

由于BP神经网络的强大表示能力，其训练过程经常会过拟合，即训练误差持续降低，但测试误差却可能上升。

解决策略:

1. 早停(early stopping):若训练集误差持续下降但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。
2. 正则化("regularization"):其基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权与阈值的平方和。仍令$E_{k}$表示第k个训练样本上的误差，$w_{i}$表示连接权和阈值，则误差目标函数改变为：
$$ E = \lambda \cfrac{1}{m} \sum_{k = 1}^m E_{k} + (1 - \lambda) \sum_{i} w_{i}^2 $$ 

- 全局最小与局部最小

    由于负梯度方向是函数值下降最快的地方，因此梯度下降法就是沿着负梯度方向搜索最优解。若误差函数在当前点的梯度为零，则已达到局部极小，更新量将为零，这意味着参数的迭代更新将在此停止。

    我们在参数寻优过程中是希望找到全局最小。

    如果误差函数仅有一个局部极小，那么此时找到的局部极小就是全局最小；然而，如果误差函数具有多个局部极小，则不能保证找到的解是全局最小。

    “跳出”局部极小的策略：
    - 以多组不同参数值初始化同一个神经网络，按标准方法训练后，取其中验证误差最小的解作为最终参数。(从多个局部极小值中选取最小值。)
  
    - 使用“模拟退火”(simulated annealing)技术：

        模拟退火在每一步都有一定的概率接收比当前解更差的结果，从而有助于“跳出”局部极小，在每一次迭代过程中，接受“次优解”的概率要随着时间的推移而逐渐降低，从而保证算法稳定。
    
    - 使用随机梯度下降：

        与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加入了随机因素。于是，即便陷入局部极小点，它计算出的梯度仍有可能不为零，这样就有机会跳出局部极小继续搜索。
    
    此外，遗传算法(genetic algorithms)也常用来训练神经网络以更好地逼近全局最小。需要注意的是，上述用于跳出局部极小的技术大多是启发式，理论上尚缺乏保障。

## 支持向量机(SVM)
待整理...


## 贝叶斯分类器
待整理...

## 集成学习

集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛华性能。

要获得好多集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”(diversity)，即学习器间具有差异。

目前的集成学习方法大致分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，代表方法为Boosting；以及个体学习器间不存在强依赖关系、可同时生成的并行化方法,代表为Bagging和“随机森林”(Random Forest)。

### Boosting

Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作即指类似：先从初始训练集中训练初一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此反复重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当若的学习器构建出很强的集成。

### Bagging与随机森林




